from typing import (
    AsyncIterator,
    Any,
    Callable,
    Dict,
    List,
    Optional,
    Tuple,
)

from llm_tools.tokens import (
    TokenExpense,
    TokenExpenses,
)
from llm_tools.chat_message import OpenAIChatMessage

from llm_tools.errors import (
    should_fallback_to_other_model,
    MultipleException,
)
from llm_tools.llm_streaming import StreamingChatModel
from llm_tools.llm_streaming_base import StreamingLLMBase



class StreamingModelWithFallback(StreamingLLMBase):
    def __init__(
        self,
        models: List[StreamingChatModel],
        should_fallback_to_other_model: Callable[[Exception], bool] = \
            should_fallback_to_other_model,
    ):
        self.models = models
        self.should_fallback_to_other_model = should_fallback_to_other_model
        self.exceptions = []
        self._successful_model_index: Optional[int] = None

    @property
    def successful_model_index(self) -> Optional[int]:
        """Index of the model that successfully completed the request."""
        return self._successful_model_index

    @property
    def successful_model(self) -> Optional[StreamingChatModel]:
        """The model that successfully completed the request."""
        if self._successful_model_index is not None:
            return self.models[self._successful_model_index]
        return None

    @property
    def successful_model_name(self) -> Optional[str]:
        """Name of the model that successfully completed the request."""
        model = self.successful_model
        return model.model if model else None

    @property
    def num_fallbacks(self) -> int:
        """Number of fallbacks that occurred (0 if first model succeeded)."""
        if self._successful_model_index is not None:
            return self._successful_model_index
        return len(self.exceptions)

    async def stream_llm_reply(
        self,
        messages: List[OpenAIChatMessage],
        stop: Optional[List[str]] = None,
    ) -> AsyncIterator[Tuple[str, str]]:
        self.exceptions = []
        self._successful_model_index = None
        for model_idx, model in enumerate(self.models):
            try:
                async for completion, token in model.stream_llm_reply(messages, stop):
                    yield completion, token
            except Exception as e:
                if self.should_fallback_to_other_model(e):
                    self.exceptions.append(e)
                    continue
                else:
                    raise
            else:
                self._successful_model_index = model_idx
                break
        else:
            if len(self.exceptions) == 1:
                raise self.exceptions[0]
            else:
                raise MultipleException(self.exceptions) from self.exceptions[-1]

    @property
    def succeeded(self) -> bool:
        return any(model.succeeded for model in self.models)

    def get_tokens_spent(
        self,
        only_successful_trial: bool = False,
    ) -> TokenExpenses:

        if not self.succeeded and only_successful_trial:
            raise ValueError("Cannot get tokens spent for unsuccessful trial")

        if only_successful_trial:
            first_successful_model = next(model for model in self.models if model._succeeded)
            return first_successful_model.get_tokens_spent(only_successful_trial)
        else:
            return sum(
                (
                    model.get_tokens_spent(only_successful_trial)
                    for model in self.models
                ),
                TokenExpenses()
            )
